{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully written to 'preprocessed_reviews_train_val.json' and 'preprocessed_reviews_test.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "# Paths to input and output files\n",
    "input_file = './data/All_beauty_more_than_3_with_product.jsonl'  # Your JSONL file\n",
    "pkl_file_path = 'data/dataset.pkl'  # Your retrieved.pkl file\n",
    "\n",
    "# Output files\n",
    "output_file_train_val = 'preprocessed_reviews_train_val.json'\n",
    "output_file_test = 'preprocessed_reviews_test.json'\n",
    "\n",
    "# Step 1: Load retrieved.pkl data\n",
    "def load_retrieved_data(pkl_file_path):\n",
    "    with open(pkl_file_path, 'rb') as f:\n",
    "        retrieved_data = pickle.load(f)\n",
    "    return retrieved_data\n",
    "\n",
    "retrieved_data = load_retrieved_data(pkl_file_path)\n",
    "\n",
    "# Extract necessary data\n",
    "train = retrieved_data['train']\n",
    "val = retrieved_data['val']\n",
    "test = retrieved_data['test']\n",
    "umap = retrieved_data['umap']\n",
    "smap = retrieved_data['smap']\n",
    "\n",
    "# Step 2: Create sets of ASINs for train+val and test sequences\n",
    "def get_sequence_item_ids(sequences):\n",
    "    item_ids = set()\n",
    "    for seq in sequences.values():\n",
    "        item_ids.update(seq)\n",
    "    return item_ids\n",
    "\n",
    "# Collect item IDs for train+val sequences\n",
    "sequence_item_ids_train_val = get_sequence_item_ids(train)\n",
    "sequence_item_ids_train_val.update(get_sequence_item_ids(val))\n",
    "\n",
    "# Collect item IDs for test sequences\n",
    "sequence_item_ids_test = get_sequence_item_ids(test)\n",
    "\n",
    "# Map item IDs to ASINs using smap\n",
    "id_to_asin = {v: k for k, v in smap.items()}\n",
    "\n",
    "def map_item_ids_to_asins(item_ids, id_to_asin):\n",
    "    asins = set()\n",
    "    for item_id in item_ids:\n",
    "        asin = id_to_asin.get(item_id)\n",
    "        if asin:\n",
    "            asins.add(asin)\n",
    "        else:\n",
    "            print(f\"Item ID {item_id} not found in smap.\")\n",
    "    return asins\n",
    "\n",
    "# ASINs for train+val and test sequences\n",
    "sequence_asins_train_val = map_item_ids_to_asins(sequence_item_ids_train_val, id_to_asin)\n",
    "sequence_asins_test = map_item_ids_to_asins(sequence_item_ids_test, id_to_asin)\n",
    "\n",
    "# Step 3: Create sets of user IDs for train+val and test\n",
    "def get_sequence_user_ids(sequences, reverse_umap):\n",
    "    user_ids = set()\n",
    "    for user_index in sequences.keys():\n",
    "        user_id = reverse_umap.get(user_index)\n",
    "        if user_id:\n",
    "            user_ids.add(user_id)\n",
    "        else:\n",
    "            print(f\"User index {user_index} not found in reverse umap.\")\n",
    "    return user_ids\n",
    "\n",
    "# Reverse umap to map indices back to user IDs\n",
    "reverse_umap = {v: k for k, v in umap.items()}\n",
    "\n",
    "# User IDs for train+val and test sequences\n",
    "user_ids_train_val = get_sequence_user_ids(train, reverse_umap)\n",
    "user_ids_train_val.update(get_sequence_user_ids(val, reverse_umap))\n",
    "\n",
    "user_ids_test = get_sequence_user_ids(test, reverse_umap)\n",
    "\n",
    "# Step 4: Process the JSONL file with filtering\n",
    "user_reviews_train_val = defaultdict(list)\n",
    "user_reviews_test = defaultdict(list)\n",
    "\n",
    "# Open and read the JSONL file\n",
    "with open(input_file, 'r', encoding='utf-8') as jsonlfile:\n",
    "    for line in jsonlfile:\n",
    "        # Parse each line as a JSON object\n",
    "        json_data = json.loads(line)\n",
    "        \n",
    "        user_id = json_data['user_id']\n",
    "        parent_asin = json_data.get('parent_asin')\n",
    "        \n",
    "        # Check if the review belongs to train+val\n",
    "        if user_id in user_ids_train_val and parent_asin in sequence_asins_train_val:\n",
    "            # Extract the relevant fields from the review\n",
    "            review = {\n",
    "                'product_name': json_data.get('product_name', 'Unknown'),\n",
    "                'parent_asin': parent_asin,\n",
    "                'rating': json_data.get('rating'),\n",
    "                'title': json_data.get('title'),\n",
    "                'text': json_data.get('text'),\n",
    "                'timestamp': json_data.get('timestamp'),\n",
    "            }\n",
    "            # Group reviews by user_id\n",
    "            user_reviews_train_val[user_id].append(review)\n",
    "        \n",
    "        # Check if the review belongs to test\n",
    "        elif user_id in user_ids_test and parent_asin in sequence_asins_test:\n",
    "            # Extract the relevant fields from the review\n",
    "            review = {\n",
    "                'product_name': json_data.get('product_name', 'Unknown'),\n",
    "                'parent_asin': parent_asin,\n",
    "                'rating': json_data.get('rating'),\n",
    "                'title': json_data.get('title'),\n",
    "                'text': json_data.get('text'),\n",
    "                'timestamp': json_data.get('timestamp'),\n",
    "            }\n",
    "            # Group reviews by user_id\n",
    "            user_reviews_test[user_id].append(review)\n",
    "        # Else, ignore the review\n",
    "\n",
    "# Step 5: Sort reviews by timestamp for each user\n",
    "for user_id, reviews in user_reviews_train_val.items():\n",
    "    user_reviews_train_val[user_id] = sorted(reviews, key=lambda x: x['timestamp'])\n",
    "\n",
    "for user_id, reviews in user_reviews_test.items():\n",
    "    user_reviews_test[user_id] = sorted(reviews, key=lambda x: x['timestamp'])\n",
    "\n",
    "# Step 6: Create the final structured outputs\n",
    "output_data_train_val = [{'user_id': user_id, 'reviews': reviews} for user_id, reviews in user_reviews_train_val.items()]\n",
    "output_data_test = [{'user_id': user_id, 'reviews': reviews} for user_id, reviews in user_reviews_test.items()]\n",
    "\n",
    "# Write the processed data to new JSON files\n",
    "with open(output_file_train_val, 'w', encoding='utf-8') as outfile_train_val:\n",
    "    json.dump(output_data_train_val, outfile_train_val, indent=4)\n",
    "\n",
    "with open(output_file_test, 'w', encoding='utf-8') as outfile_test:\n",
    "    json.dump(output_data_test, outfile_test, indent=4)\n",
    "\n",
    "print(f\"Data has been successfully written to '{output_file_train_val}' and '{output_file_test}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "# Load retrieved.pkl data\n",
    "def load_retrieved_data(pkl_file_path):\n",
    "    with open(pkl_file_path, 'rb') as f:\n",
    "        retrieved_data = pickle.load(f)\n",
    "    return retrieved_data\n",
    "\n",
    "pkl_file_path = 'data/dataset.pkl'  # Update with your actual path\n",
    "retrieved_data = load_retrieved_data(pkl_file_path)\n",
    "\n",
    "train = retrieved_data['train']\n",
    "val = retrieved_data['val']\n",
    "test = retrieved_data['test']\n",
    "umap = retrieved_data['umap']\n",
    "smap = retrieved_data['smap']\n",
    "meta = retrieved_data['meta']\n",
    "\n",
    "# Load processed JSON files\n",
    "processed_train_val_file = 'preprocessed_reviews_train_val.json'\n",
    "processed_test_file = 'preprocessed_reviews_test.json'\n",
    "\n",
    "with open(processed_train_val_file, 'r', encoding='utf-8') as f:\n",
    "    processed_train_val_data = json.load(f)\n",
    "\n",
    "with open(processed_test_file, 'r', encoding='utf-8') as f:\n",
    "    processed_test_data = json.load(f)\n",
    "\n",
    "# Prepare mappings\n",
    "index_to_user_id = {v: k for k, v in umap.items()}\n",
    "item_id_to_asin = {v: k for k, v in smap.items()}\n",
    "asin_to_item_id = {k: v for v, k in smap.items()}\n",
    "\n",
    "# Define functions\n",
    "def get_user_sequences(sequences, index_to_user_id):\n",
    "    user_sequences = {}\n",
    "    for user_index, item_ids in sequences.items():\n",
    "        user_id = index_to_user_id.get(user_index)\n",
    "        if user_id:\n",
    "            user_sequences[user_id] = item_ids\n",
    "        else:\n",
    "            print(f\"User index {user_index} not found in umap.\")\n",
    "    return user_sequences\n",
    "\n",
    "def get_reviews_by_user(processed_data):\n",
    "    reviews_by_user = {}\n",
    "    for user_data in processed_data:\n",
    "        user_id = user_data['user_id']\n",
    "        reviews_by_user[user_id] = user_data['reviews']\n",
    "    return reviews_by_user\n",
    "\n",
    "def compare_user_data(user_sequences, reviews_by_user, item_id_to_asin):\n",
    "    discrepancies = {}\n",
    "    for user_id, item_ids in user_sequences.items():\n",
    "        # Get ASINs from item IDs\n",
    "        asins_from_sequences = set()\n",
    "        for item_id in item_ids:\n",
    "            asin = item_id_to_asin.get(item_id)\n",
    "            if asin:\n",
    "                asins_from_sequences.add(asin)\n",
    "            else:\n",
    "                print(f\"Item ID {item_id} not found in smap.\")\n",
    "        \n",
    "        # Get ASINs from user's reviews\n",
    "        user_reviews = reviews_by_user.get(user_id, [])\n",
    "        asins_from_reviews = set(review['parent_asin'] for review in user_reviews)\n",
    "        \n",
    "        # Compare the ASIN sets\n",
    "        missing_in_reviews = asins_from_sequences - asins_from_reviews\n",
    "        extra_in_reviews = asins_from_reviews - asins_from_sequences\n",
    "        \n",
    "        if missing_in_reviews or extra_in_reviews:\n",
    "            discrepancies[user_id] = {\n",
    "                'missing_in_reviews': missing_in_reviews,\n",
    "                'extra_in_reviews': extra_in_reviews\n",
    "            }\n",
    "    \n",
    "    return discrepancies\n",
    "\n",
    "def report_discrepancies(discrepancies, dataset_name):\n",
    "    if discrepancies:\n",
    "        print(f\"\\nDiscrepancies found in {dataset_name}:\")\n",
    "        for user_id, discrepancy in discrepancies.items():\n",
    "            print(f\"User ID: {user_id}\")\n",
    "            if discrepancy['missing_in_reviews']:\n",
    "                print(f\"  Items in sequences but missing in reviews (ASINs): {discrepancy['missing_in_reviews']}\")\n",
    "            if discrepancy['extra_in_reviews']:\n",
    "                print(f\"  Items in reviews but not in sequences (ASINs): {discrepancy['extra_in_reviews']}\")\n",
    "    else:\n",
    "        print(f\"\\nNo discrepancies found in {dataset_name}. All items match.\")\n",
    "\n",
    "# Retrieve user sequences\n",
    "train_val_sequences = {**train, **val}\n",
    "user_sequences_train_val = get_user_sequences(train_val_sequences, index_to_user_id)\n",
    "user_sequences_test = get_user_sequences(test, index_to_user_id)\n",
    "\n",
    "# Retrieve reviews by user\n",
    "reviews_by_user_train_val = get_reviews_by_user(processed_train_val_data)\n",
    "reviews_by_user_test = get_reviews_by_user(processed_test_data)\n",
    "\n",
    "# Compare data\n",
    "discrepancies_train_val = compare_user_data(user_sequences_train_val, reviews_by_user_train_val, item_id_to_asin)\n",
    "discrepancies_test = compare_user_data(user_sequences_test, reviews_by_user_test, item_id_to_asin)\n",
    "\n",
    "# Report discrepancies\n",
    "report_discrepancies(discrepancies_train_val, 'Train + Validation Data')\n",
    "report_discrepancies(discrepancies_test, 'Test Data')\n",
    "\n",
    "# Verify counts\n",
    "def get_counts(user_sequences):\n",
    "    num_users = len(user_sequences)\n",
    "    num_interactions = sum(len(items) for items in user_sequences.values())\n",
    "    unique_items = set()\n",
    "    for items in user_sequences.values():\n",
    "        unique_items.update(items)\n",
    "    num_items = len(unique_items)\n",
    "    return num_users, num_interactions, num_items\n",
    "\n",
    "num_users_train_val, num_interactions_train_val, num_items_train_val = get_counts(user_sequences_train_val)\n",
    "num_users_test, num_interactions_test, num_items_test = get_counts(user_sequences_test)\n",
    "\n",
    "def get_processed_counts(reviews_by_user):\n",
    "    num_users = len(reviews_by_user)\n",
    "    num_interactions = sum(len(reviews) for reviews in reviews_by_user.values())\n",
    "    unique_asins = set()\n",
    "    for reviews in reviews_by_user.values():\n",
    "        unique_asins.update(review['parent_asin'] for review in reviews)\n",
    "    num_items = len(unique_asins)\n",
    "    return num_users, num_interactions, num_items\n",
    "\n",
    "num_users_processed_train_val, num_interactions_processed_train_val, num_items_processed_train_val = get_processed_counts(reviews_by_user_train_val)\n",
    "num_users_processed_test, num_interactions_processed_test, num_items_processed_test = get_processed_counts(reviews_by_user_test)\n",
    "\n",
    "# Print counts\n",
    "print(\"\\nCounts in retrieved.pkl (Train + Val):\")\n",
    "print(f\"Number of users: {num_users_train_val}\")\n",
    "print(f\"Number of interactions: {num_interactions_train_val}\")\n",
    "print(f\"Number of unique items: {num_items_train_val}\")\n",
    "\n",
    "print(\"\\nCounts in processed_reviews_train_val.json:\")\n",
    "print(f\"Number of users: {num_users_processed_train_val}\")\n",
    "print(f\"Number of interactions: {num_interactions_processed_train_val}\")\n",
    "print(f\"Number of unique items: {num_items_processed_train_val}\")\n",
    "\n",
    "print(\"\\nCounts in retrieved.pkl (Test):\")\n",
    "print(f\"Number of users: {num_users_test}\")\n",
    "print(f\"Number of interactions: {num_interactions_test}\")\n",
    "print(f\"Number of unique items: {num_items_test}\")\n",
    "\n",
    "print(\"\\nCounts in processed_reviews_test.json:\")\n",
    "print(f\"Number of users: {num_users_processed_test}\")\n",
    "print(f\"Number of interactions: {num_interactions_processed_test}\")\n",
    "print(f\"Number of unique items: {num_items_processed_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
